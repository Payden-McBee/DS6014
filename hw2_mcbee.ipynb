{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Payden McBee\n",
    "## HW2: Bayes Optimal Classifiers\n",
    "### Due: 1400 on 5 Oct 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue\"> 1. (40) The authors in [1] describe a marketing campaign by a bank in Portugal.\n",
    "Modify the Exercise3.2LDAQDA IrisSoution python code to use the\n",
    "data (bank-full.csv) from this marketing campaign with only the predictor\n",
    "variables age, balance, and duration and the response variable, y, to\n",
    "create the Bayes optimal classifiers for each of the conditions listed below.\n",
    "Provide your python code for each of these cases.\n",
    "  <br>  (a) Assume Gaussian class conditional likelihoods with unequal variancecovariance\n",
    "matrices with each of the following additional assumptions\n",
    "applied singularly to each decision rule in this class:\n",
    "    <br> i. Equal class priors and equal costs for misclassification;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, I upload the libraries I need and read the \"bank-full.csv\" into a dataframe. Then, I pull out the columns of interest, age, balance, and duration, and the response variable, y. I rename the  response variable as class and save it to a csv file. I did this because LDA/QDA files take a .csv file as input and expect the response column to be labelled class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys,os\n",
    "from io import StringIO\n",
    "import QDA\n",
    "import LDA\n",
    "bank = pd.read_csv(\"bank-full.csv\")\n",
    "bank_r = bank[['age', 'balance', 'duration','y']].copy()\n",
    "bank_r.rename({'y': 'Class'}, axis=1, inplace=True)\n",
    "#bank_r.to_csv('bank_r.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)i) Since I am assuming Gaussian class conditional likelihoods with unequal variancecovariance matrices, I use the QDA model. I use the QDA class as provided without altering it. Then, I create the uninformative_priors variable for the equal class priors and equal_costs to both be 1 for misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qda = QDA.QDA('bank_r.csv')\n",
    "uninformative_priors = {\n",
    "    \"yes\": 1 / 2,\n",
    "    \"no\": 1 / 2\n",
    "}\n",
    "equal_costs = {\n",
    "    \"yes\": 1,\n",
    "    \"no\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then create the compute_probabilities and computeAccuracy functions as below, and then implement them to find the overall accuracy and the accuracy per class. The compute_probabilities function is called in the computeAccuracy function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " def compute_probabilities(self, x, priors):\n",
    "        # compute and output the probability of each class and the maximum probability class\n",
    "\n",
    "        # check that the input data x has the correct number of rows\n",
    "        if not (len(x) == self.num_cols):\n",
    "            print('Data vector has wrong number of values.')\n",
    "            return -1\n",
    "\n",
    "        # reformat x as a numpy array, incase the user input a list\n",
    "        x = np.asarray(x)\n",
    "\n",
    "        # first compute the likelihood of each class\n",
    "        likelihoods = np.zeros(len(self.class_names))\n",
    "        idx = 0\n",
    "        for name in self.class_names:\n",
    "            likelihoods[idx] = multivariate_gaussian_pdf(x, self.means[name], self.covs[name])\n",
    "            idx = idx + 1\n",
    "\n",
    "        # compute the probabilities of each class\n",
    "        probabilites = np.zeros(len(self.class_names))\n",
    "        idx = 0\n",
    "        for name in self.class_names:\n",
    "            probabilites[idx] = likelihoods[idx]*priors[name]#multivariate_gaussian_pdf(x, self.means[name], self.cov)\n",
    "            idx = idx + 1\n",
    "        probabilites = probabilites/probabilites.sum()\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " def computeAccuracy(self,priors,costsD):\n",
    "        costs = np.zeros(self.class_names.shape)\n",
    "        idx = 0\n",
    "        for name in self.class_names:\n",
    "            costs[idx] = costsD[name]\n",
    "            idx=idx+1\n",
    "        predClass = ['' for i in range(self.num_rows)]\n",
    "        idx = 0\n",
    "        for x in self.data:\n",
    "            predClass[idx] = self.class_names[\n",
    "                np.argmax(costs*self.compute_probabilities(x, priors), axis=0)]\n",
    "            idx = idx + 1\n",
    "        idx = 0\n",
    "        accuracy = 0\n",
    "        tp_PerClass = {classLabel: 0 for classLabel in self.data_labels}\n",
    "\n",
    "        for y in self.data_labels:\n",
    "            if y == predClass[idx]:\n",
    "                accuracy = accuracy + 1\n",
    "                tp_PerClass[y] = tp_PerClass[y] + 1\n",
    "            idx = idx + 1\n",
    "        accuracy = accuracy /  self.num_rows\n",
    "        acc_PerClass = {label:tp_PerClass[label] / self.num_obs[label] for label in tp_PerClass}\n",
    "        return accuracy, acc_PerClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA, uninformative priors, equal costs\n",
      "0.8373183517285616\n",
      "{'no': 0.8407394419117279, 'yes': 0.8114955568160332}\n"
     ]
    }
   ],
   "source": [
    "accuracy, acc_PerClass = \\\n",
    "model_qda.computeAccuracy(uninformative_priors,equal_costs)\n",
    "print(\"QDA, uninformative priors, equal costs\")\n",
    "print(accuracy)\n",
    "print(acc_PerClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I computed the overall accuracy to be about 84% and the accuracy for no and yes to be 84% and 81%, respecively.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)a)ii) Next, I compute the accuracies for a prior of 0.9 for not selecting the new bank service and the same equal misclassification costs are before. I  use the same computeAccuracy function as above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA, informative priors, equal costs\n",
      "0.8852712835371923\n",
      "{'no': 0.9609488502580031, 'yes': 0.3140480242011722}\n"
     ]
    }
   ],
   "source": [
    "informative_priors = {\n",
    "    \"yes\": 1 / 10,\n",
    "    \"no\": 9/10\n",
    "}\n",
    "\n",
    "accuracy, acc_PerClass = model_qda.computeAccuracy(informative_priors,equal_costs)\n",
    "print(\"QDA, informative priors, equal costs\")\n",
    "print(accuracy)\n",
    "print(acc_PerClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the informative prior, the overall classification accuracy has risen to approximately 86%. However, when I inspect the per class accuracy, I see that the gains have come from the no class, rising to 96%. However, the correct classification for the yes class has dropped to 31.4%. This is due to class imbalance. Since the no class is much more likely, if we guess no more often, we're likely to have a higher accuracy. I show the class imbalance below, where the yes  response makes up 12% and the no response 88% of the total responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes 12.0 %\n",
      "no 88.0 %\n"
     ]
    }
   ],
   "source": [
    "print('yes',np.round(100*sum(model_qda.data_labels == 'yes')/model_qda.num_rows),'%')\n",
    "print('no',np.round(100*sum(model_qda.data_labels == 'no')/model_qda.num_rows),'%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)a)iii) I now compute the accuracy for a prior of 0.9 for not selecting the new bank service and the cost of classifying a customer as not a new service candidate when there are as 15 times the cost of classifying a customer as a new service customer when they are not. I run the computeAccuracy function as implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA, informative priors, unequal costs\n",
      "0.7630665103625224\n",
      "{'no': 0.7470567606833325, 'yes': 0.8839100018907166}\n"
     ]
    }
   ],
   "source": [
    "uneq_costs = {\n",
    "    \"yes\" : 15,\n",
    "    \"no\" : 1\n",
    "}\n",
    "accuracy, acc_PerClass = model_qda.computeAccuracy(informative_priors,uneq_costs)\n",
    "print(\"QDA, informative priors, unequal costs\")\n",
    "print(accuracy)\n",
    "print(acc_PerClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall classificaiton accuracy and the no classification accuracy went down to 76% and 75%, respectively. The accuracy for the yes response rose to 88%, the highest of all the 3 scenarios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) b)i) Since I am assuming Gaussian class conditional likelihoods with equal variancecovariance matrices, I use the LDA model. I use the LDA class as provided without altering it. Then, I use the uninformative_priors variable for the equal class priors and equal_costs variable for misclassification, as descrived in Part (1i). I use create the compute_probabilities and computeAccuracy functions, as implemented above, to find the overall accuracy and the accuracy per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA, uninformative priors, equal costs\n",
      "Overall Accuracy 85.0 %\n",
      "{'no': 0.851886178047192, 'yes': 0.7982605407449423}\n"
     ]
    }
   ],
   "source": [
    "model_lda = LDA.LDA('bank_r.csv')\n",
    "#part i\n",
    "accuracy, acc_PerClass = model_lda.computeAccuracy(uninformative_priors,equal_costs)\n",
    "print(\"LDA, uninformative priors, equal costs\")\n",
    "print('Overall Accuracy',np.round(100*accuracy),'%')\n",
    "print(acc_PerClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall classification accuracy is 85%, and the accuracy for no and yes to be 85% and 85%, respecively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)b)ii) Next, I compute the accuracies for a prior of 0.9 for not selecting the new bank service and the same equal misclassification costs are before. I  use the same computeAccuracy function as above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA, informative priors, equal costs\n",
      "Overall Accuracy 89.0 %\n",
      "{'no': 0.9753769851209859, 'yes': 0.27982605407449423}\n"
     ]
    }
   ],
   "source": [
    "accuracy, acc_PerClass = model_lda.computeAccuracy(informative_priors,equal_costs)\n",
    "print(\"LDA, informative priors, equal costs\")\n",
    "print('Overall Accuracy',np.round(100*accuracy),'%')\n",
    "print(acc_PerClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the informative priors, the overall accuracy and no response accuracy increases, as expected, though higher than in the case of LDA. Again, as expected, the accuracy for the yes class drops due to the class imbalance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)b)iii I now compute the accuracy for a prior of 0.9 for not selecting the new bank service and the cost of classifying a customer as not a new service candidate when there are as 15 times the cost of classifying a customer as a new service customer when they are not. I run the computeAccuracy function as implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA, informative priors, unequal costs\n",
      "0.7814691114994139\n",
      "{'no': 0.7660688342267421, 'yes': 0.8977122329362829}\n"
     ]
    }
   ],
   "source": [
    "accuracy, acc_PerClass = model_lda.computeAccuracy(informative_priors,uneq_costs)\n",
    "print(\"LDA, informative priors, unequal costs\")\n",
    "print(accuracy)\n",
    "print(acc_PerClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall classificaiton accuracy and the no classification accuracy went down to 78% and 77%, respectively. The accuracy for the yes response rose to 89.8%, the highest of all the 3 scenarios. These accuracies are also higher than the QDA model, though not by much. It's possible that the QDA model overfit the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue\"> 2) (25) Use numpy and pandas to develop a Naive Bayes classifier for edible mushrooms with the data in MushroomData.csv and MushroomVariablestxt. Use 2/3 of the observations to train the classifier and test on 1/3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, I read in the Mushroom.csv file into a pandas dataframe. I drop the last row, which is full of NaNs. I also read in the MushroomVariables.txt and set them as the column labels for the mushroom dataframe. I drop the class label, the first column, to create categories variable. This is a list of the feature variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mushroom = pd.read_csv(\"MushroomData.csv\",header=None)\n",
    "mushroom.drop([mushroom.shape[0]-1],inplace=True) #get rid of NaN in last row\n",
    "m_vars = pd.read_csv('MushroomVariables.txt', sep=\",\", header=None,dtype=str)\n",
    "categories = m_vars.drop(columns=[0])\n",
    "mushroom.columns = m_vars.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I add the variable name to the word in each column. This is because some words are repeated in the different columns, but they do not carry the same meaning. For example, the color for the cap-color variable and for the veil-color may both be white, but they refer to different features. Furthermore, I keep the None's, since they are informative of the absense of a feature, such as the ring-number.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dictionary= []\n",
    "for cat in categories:\n",
    "    mushroom[categories[cat][0]]= mushroom[categories[cat][0]].apply(lambda word:word+categories[cat][0])\n",
    "    Dictionary.append(mushroom[categories[cat][0]].unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I separate the mushroom dataframe into the edible and nonedible classes. Then I put 2/3 of each of the classes into the training set and 1/3 into the test set, thus having an equal balance of classes in the training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "edible_df = mushroom[mushroom['edible_class'] == 'EDIBLE']\n",
    "nonedible_df = mushroom[mushroom['edible_class'] != 'EDIBLE']\n",
    "tng_ed_msk = np.random.rand(len(edible_df)) < 0.67\n",
    "tst_ed_msk = ~tng_ed_msk\n",
    "tng_ned_msk = np.random.rand(len(nonedible_df)) < 0.67\n",
    "tst_ned_msk = ~tng_ned_msk\n",
    "train_frames = [edible_df[tng_ed_msk], nonedible_df[tng_ned_msk]]\n",
    "trainSet = pd.concat(train_frames)\n",
    "test_frames = [edible_df[tst_ed_msk], nonedible_df[tst_ned_msk]]\n",
    "testSet = pd.concat(test_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create a dictionary, prior, containing the number of each class' occurence. I also create the likelihood and evidence dictionaries that hold a probability for each word given the class. I add in a small term, alpha, to keep the likelihood of a word given a class from being equal to zero. I also normalize by the number of unique words in a feature multiplied by alpha. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "prior = {label:trainSet[trainSet['edible_class'] == label].shape[0] for label in trainSet['edible_class'].unique()}\n",
    "likelihood = dict.fromkeys([item for sublist in Dictionary for item in sublist],[0,0])\n",
    "evidence = dict.fromkeys([item for sublist in Dictionary for item in sublist],0)\n",
    "numTrngObs = trainSet.shape[0]\n",
    "numEdTrngObs = sum(trainSet['edible_class'] == 'EDIBLE')\n",
    "numNonedTrngObs = sum(trainSet['edible_class'] == 'POISONOUS')\n",
    "alpha = 0.01\n",
    "for cat in categories:\n",
    "    for word in Dictionary[cat-1]:\n",
    "        evidence[word] = sum(trainSet[categories[cat][0]] == word)/numTrngObs\n",
    "        if sum(trainSet[categories[cat][0]] == word) >0:\n",
    "            likelihood[word] =  ((sum((trainSet[categories[cat][0]] == word) & (trainSet['edible_class'] == 'EDIBLE'))+alpha)/(numEdTrngObs+alpha*len(Dictionary[cat-1])),(sum((trainSet[categories[cat][0]] == word) & (trainSet['edible_class'] == 'POISONOUS'))+alpha)/(numNonedTrngObs+alpha*len(Dictionary[cat-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create the prior probabilities for each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priorProbs = np.array([prior['EDIBLE']/numTrngObs,prior['POISONOUS']/numTrngObs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create a function to predict the label for each row given the features, likelihood, evidence and prior probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataSet,likelihood,evidence,priorProbs):\n",
    "    numObs = dataSet.shape[0]\n",
    "    predClass = ['' for i in range(numObs)]\n",
    "\n",
    "    dataF = np.asarray(dataSet.drop('edible_class', axis=1, inplace=False))\n",
    "    data_labels = dataSet.loc[:]['edible_class']\n",
    "\n",
    "    class_names = np.unique(data_labels)\n",
    "\n",
    "    numClasses = class_names.shape[0]\n",
    "    x_idx = 0\n",
    "    for x in dataF:\n",
    "        probRow = np.zeros(shape=(dataF.shape[1], numClasses))\n",
    "        w = 0\n",
    "        for word in x:\n",
    "            for label in range(numClasses):\n",
    "                probRow[w, label] = likelihood[word][label] / evidence[word]\n",
    "            w = w + 1\n",
    "        predClass[x_idx] = class_names[np.argmax(np.prod(probRow, axis=0) * priorProbs)]\n",
    "        x_idx = x_idx + 1\n",
    "    return predClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the predict function to predict the labels and computeAccuracies function to compare the predicted labels to the true labels. I first do this for the training dataset to gauge performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsTrain= predict(trainSet,likelihood,evidence,priorProbs) #priorProbs is  learned from training  set,  same for train and test\n",
    "data_labels_train = trainSet.loc[:]['edible_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAccuracies(data_labels,predClass,priorNum):\n",
    "    accuracy = 0\n",
    "    numObs = data_labels.shape[0]\n",
    "    tp_PerClass = {classLabel: 0 for classLabel in data_labels}\n",
    "    idx = 0\n",
    "    for y in data_labels:\n",
    "        if y == predClass[idx]:\n",
    "            accuracy = accuracy + 1\n",
    "            tp_PerClass[y] = tp_PerClass[y] + 1\n",
    "        idx = idx + 1\n",
    "    accuracy = accuracy / numObs\n",
    "    acc_PerClass = {label: tp_PerClass[label] / priorNum[label] for label in tp_PerClass}\n",
    "    return accuracy, acc_PerClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9903982930298719\n",
      "{'EDIBLE': 0.9949799196787149, 'POISONOUS': 0.9852048558421851}\n"
     ]
    }
   ],
   "source": [
    "accuracy, acc_PerClass=computeAccuracies(data_labels_train,predictionsTrain,prior)#for test make sure to use prior num in test set, not same as train set, same for data_labels\n",
    "print(accuracy)\n",
    "print(acc_PerClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall accuracy for the training set is high, at 99%. The accuracies for the EDIBLE and POINSONOUS classes are 99.5% and 98.5%, respectively. I then predict the label and compute the accuracies for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9949856733524355\n",
      "{'EDIBLE': 0.9966666666666667, 'POISONOUS': 0.9930340557275542}\n"
     ]
    }
   ],
   "source": [
    "predictionsTest = predict(testSet,likelihood,evidence,priorProbs)\n",
    "\n",
    "data_labels_test = testSet.loc[:]['edible_class']\n",
    "Test_prior = {label:testSet[testSet['edible_class'] == label].shape[0] for label in testSet['edible_class'].unique()}\n",
    "test_accuracy, test_acc_PerClass = computeAccuracies(data_labels_test,predictionsTest,Test_prior)\n",
    "print(test_accuracy)\n",
    "print(test_acc_PerClass)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall accuracy and accuracies for each class are above 99%. The model generalized well to the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
